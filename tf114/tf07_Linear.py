#y = wx + b  구현 하기 
# w,b = 변수 / x, y = place holder (입력값은 p.h) / 완전한 고정값은 상수 

import tensorflow as tf     
tf.set_random_seed(66)

x_train = [1,2,3] #w = 1, b =0 인걸 우리는 알지만 머신은 모른다
y_train = [1,2,3]

W = tf.Variable([1], dtype=tf.float32, name='test') #[1] = 랜덤하게 내맘대로 넣은 초기값일뿐이다 
b = tf.Variable([1], dtype=tf.float32, name = 'test')

hypothesis = x_train * W + b #모델 구현 
#hypothesis는 f(x) = wx + b 표현 y 대신 사용한것 

loss = tf.reduce_mean(tf.square(hypothesis - y_train)) #mse
#reduce_mean 평균값 내는것으로 mean과같다 square = 제곱 

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) #기본 제공해주는 옵티마이저 
train = optimizer.minimize(loss) #로스값은 제일 작은 값이 좋기 때문에 로스의 최소값을 잡아주는 옵티마이저 

sess = tf.Session() 
sess.run(tf.global_variables_initializer()) #변수 초기화 

#훈련 2000번 실행한다 
for step in range(2001):
    sess.run(train)#연산 
    if step %20 ==0: #20번 돌릴때마다 한번씩 출력 된다 step을 20으로 나눌때 나머지가 0 이면 print
        print(step, sess.run(loss), sess.run(W), sess.run(b))

#step %20 ==0 -> 왼쪽 변수에서 오른쪽값을 나눈후 나머지를 반환한다

# 0 0.8110667 [0.96] [0.98]
# 20 0.09747055 [0.68079156] [0.8102924]
# 40 0.08272933 [0.66902816] [0.7604377]
# 60 0.07508357 [0.682034] [0.7235789]
# 80 0.068191625 [0.6967346] [0.6894666]
# 100 0.061932724 [0.7109639] [0.65705407]        
# 120 0.05624826 [0.7245453] [0.626174]
# 140 0.051085588 [0.7374904] [0.596746]
# 160 0.046396747 [0.7498274] [0.56870115]        
# 180 0.04213832 [0.7615845] [0.5419744]
# 200 0.038270667 [0.7727892] [0.5165037]
# 220 0.03475807 [0.7834672] [0.49222997]
# 240 0.03156784 [0.7936434] [0.46909702]
# 260 0.02867041 [0.8033415] [0.44705117]
# 280 0.02603892 [0.8125837] [0.42604136]
# 300 0.02364896 [0.8213916] [0.406019]
# 320 0.02147836 [0.82978547] [0.3869376]
# 340 0.019506995 [0.837785] [0.36875296]
# 360 0.017716562 [0.84540844] [0.35142294]       
# 380 0.016090473 [0.8526737] [0.3349073]
# 400 0.014613622 [0.85959756] [0.31916788]       
# 420 0.013272323 [0.86619586] [0.30416816]       
# 440 0.012054126 [0.87248427] [0.28987336]
# 460 0.01094775 [0.87847704] [0.2762503]
# 480 0.00994292 [0.8841882] [0.2632675]
# 500 0.009030321 [0.889631] [0.25089484]
# 520 0.00820147 [0.8948179] [0.2391037]
# 540 0.007448713 [0.899761] [0.22786674]
# 560 0.0067650415 [0.9044719] [0.21715784]       
# 580 0.0061441152 [0.90896136] [0.20695223]
# 600 0.0055801845 [0.9132399] [0.19722626]       
# 620 0.0050680176 [0.9173173] [0.18795733]       
# 640 0.0046028537 [0.9212031] [0.17912397]       
# 660 0.00418039 [0.9249062] [0.17070584]
# 680 0.0037967034 [0.9284352] [0.16268332]       
# 700 0.0034482165 [0.9317986] [0.15503784]
# 720 0.003131728 [0.9350038] [0.14775163]        
# 740 0.0028442843 [0.9380583] [0.14080784]       
# 760 0.002583229 [0.9409694] [0.13419041]        
# 780 0.002346135 [0.9437436] [0.12788399]        
# 800 0.002130795 [0.94638747] [0.12187389]       
# 820 0.0019352174 [0.9489071] [0.11614626]       
# 840 0.0017575937 [0.95130825] [0.11068777]
# 860 0.0015962725 [0.9535966] [0.10548585]       
# 880 0.00144976 [0.9557774] [0.10052839]
# 900 0.0013166973 [0.95785564] [0.09580395]      
# 920 0.0011958504 [0.95983636] [0.09130153]      
# 940 0.0010860872 [0.96172386] [0.08701065]      
# 960 0.0009863975 [0.96352273] [0.08292144]      
# 980 0.0008958689 [0.965237] [0.07902443]
# 1000 0.0008136393 [0.9668708] [0.07531057]      
# 1020 0.00073896087 [0.96842766] [0.07177125]    
# 1040 0.000671136 [0.9699114] [0.06839835]       
# 1060 0.0006095381 [0.97132546] [0.06518389]     
# 1080 0.000553593 [0.9726731] [0.06212049]       
# 1100 0.0005027807 [0.97395736] [0.05920104]
# 1120 0.00045663046 [0.9751812] [0.05641883]     
# 1140 0.00041472193 [0.9763476] [0.05376735]     
# 1160 0.00037665654 [0.9774592] [0.0512405]      
# 1180 0.0003420842 [0.97851855] [0.04883238]     
# 1200 0.00031068941 [0.9795281] [0.04653744]     
# 1220 0.00028216987 [0.98049027] [0.04435027]
# 1240 0.00025627288 [0.98140717] [0.04226596]    
# 1260 0.00023275013 [0.98228097] [0.0402796]     
# 1280 0.00021138787 [0.9831137] [0.03838658]     
# 1300 0.00019198499 [0.9839073] [0.03658255]     
# 1320 0.00017436403 [0.98466355] [0.03486332]    
# 1340 0.00015836056 [0.98538435] [0.03322484]
# 1360 0.00014382433 [0.9860712] [0.03166338]     
# 1380 0.00013062467 [0.9867258] [0.03017533]     
# 1400 0.000118634845 [0.9873496] [0.02875721]    
# 1420 0.00010774505 [0.9879442] [0.02740573]     
# 1440 9.785653e-05 [0.9885107] [0.02611776]      
# 1460 8.8874716e-05 [0.9890507] [0.02489036]
# 1480 8.0718e-05 [0.9895652] [0.02372064]        
# 1500 7.330924e-05 [0.99005574] [0.02260583]     
# 1520 6.658002e-05 [0.9905231] [0.02154339]      
# 1540 6.0469454e-05 [0.99096847] [0.0205309]     
# 1560 5.4918462e-05 [0.9913929] [0.019566]       
# 1580 4.9878097e-05 [0.9917974] [0.01864646]     
# 1600 4.530067e-05 [0.9921829] [0.01777015]
# 1620 4.114256e-05 [0.99255025] [0.01693501]     
# 1640 3.736659e-05 [0.9929004] [0.01613914]      
# 1660 3.3936896e-05 [0.99323404] [0.01538064]    
# 1680 3.0821753e-05 [0.993552] [0.0146578]       
# 1700 2.7992872e-05 [0.993855] [0.01396895]      
# 1720 2.5423387e-05 [0.9941438] [0.01331248]
# 1740 2.3090353e-05 [0.99441904] [0.01268685]    
# 1760 2.0970465e-05 [0.99468136] [0.01209062]    
# 1780 1.9045985e-05 [0.9949312] [0.01152244]     
# 1800 1.7298553e-05 [0.9951695] [0.01098093]     
# 1820 1.5710277e-05 [0.9953965] [0.01046485]     
# 1840 1.4268378e-05 [0.99561286] [0.00997303]    
# 1860 1.2959122e-05 [0.99581903] [0.00950434]
# 1880 1.1769396e-05 [0.9960155] [0.00905766]     
# 1900 1.0689179e-05 [0.99620277] [0.00863201]    
# 1920 9.708013e-06 [0.9963812] [0.00822634]      
# 1940 8.816933e-06 [0.9965513] [0.00783973]      
# 1960 8.007689e-06 [0.99671334] [0.00747131]     
# 1980 7.2730195e-06 [0.99686784] [0.00712019]    
# 2000 6.6050598e-06 [0.99701506] [0.00678556]